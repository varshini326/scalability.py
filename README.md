# scalability.py

COMPANY:CODETECH IT SOLUTIONS 
NAME:V.LAKSHMI VARSHINI 
INTERN ID :CT08FJV 
DOMAIN : DATA ANALYTICS 
BATCH DURATION :DECEMBER 25th 2024 to JANUARY 25th 2025 
MENTOR NAME : NEELA SANTHOSH

Description of the task :
Perform analysis on a large data set using tools like pyspark or dask to demonstrate scalability
 for this we need to install required libraries like pyspark or dask
 I did the task using pyspark.
 I had used the adult dataset or US census income dataset which is widely used for large-scale data processing.
 Goal is to analyze income distribution and demographics
 Initialize spark session and define the schema since the dataset has no headers.
 Load the dataset and filter the dataset for people working more than 40hrs of work.
 compute the average age and work distribution by income group and show the results.

output of the task :
 https://colab.research.google.com/drive/16PpIcuqEQyidE-AZHYFeUn_TG3TGEs7T#scrollTo=qfpBRDf0-QkQ
